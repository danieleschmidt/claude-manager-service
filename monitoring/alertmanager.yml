# Alertmanager configuration for Claude Code Manager
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@claude-manager.local'
  smtp_auth_username: 'alerts@claude-manager.local'
  smtp_auth_password: '${SMTP_PASSWORD}'
  slack_api_url: '${SLACK_WEBHOOK_URL}'
  
# Templates for alert notifications
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree for alert routing
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default'
  
  routes:
    # Critical alerts go to PagerDuty and Slack
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 10s
      repeat_interval: 5m
      continue: true
    
    # Infrastructure alerts
    - match:
        service: infrastructure
      receiver: 'infrastructure-team'
      group_wait: 30s
      repeat_interval: 30m
    
    # Database alerts
    - match_re:
        service: ^(database|postgresql|redis)$
      receiver: 'database-team'
      group_wait: 20s
      repeat_interval: 15m
    
    # GitHub API alerts
    - match:
        service: github-api
      receiver: 'dev-team'
      group_wait: 15s
      repeat_interval: 10m
    
    # Business logic alerts
    - match:
        service: business
      receiver: 'product-team'
      group_wait: 60s
      repeat_interval: 2h

# Alert receivers configuration
receivers:
  # Default receiver
  - name: 'default'
    email_configs:
      - to: 'team@claude-manager.local'
        subject: '[Claude Manager] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
          {{ end }}
  
  # Critical alerts - multiple channels
  - name: 'critical-alerts'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_ROUTING_KEY}'
        description: '{{ .GroupLabels.alertname }}'
        details:
          alert_count: '{{ len .Alerts }}'
          service: '{{ .GroupLabels.service }}'
          cluster: '{{ .GroupLabels.cluster }}'
        links:
          - href: '${GRAFANA_URL}/alerting/list'
            text: 'View in Grafana'
          - href: '${PROMETHEUS_URL}/alerts'
            text: 'View in Prometheus'
    
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#alerts-critical'
        title: 'üö® CRITICAL ALERT: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Summary:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Service:* {{ .Labels.service }}
          *Severity:* {{ .Labels.severity }}
          {{ if .Annotations.runbook_url }}*Runbook:* <{{ .Annotations.runbook_url }}|View Runbook>{{ end }}
          {{ end }}
        color: 'danger'
        send_resolved: true
  
  # Infrastructure team alerts
  - name: 'infrastructure-team'
    email_configs:
      - to: 'infrastructure@claude-manager.local'
        subject: '[Infrastructure] {{ .GroupLabels.alertname }}'
        body: |
          Infrastructure Alert Details:
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          Instance: {{ .Labels.instance }}
          {{ end }}
    
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#infrastructure'
        title: 'üõ†Ô∏è Infrastructure Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Service:* {{ .Labels.service }}
          *Instance:* {{ .Labels.instance }}
          {{ end }}
        color: 'warning'
  
  # Database team alerts
  - name: 'database-team'
    email_configs:
      - to: 'database@claude-manager.local'
        subject: '[Database] {{ .GroupLabels.alertname }}'
        body: |
          Database Alert Details:
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Database: {{ .Labels.service }}
          {{ end }}
    
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#database'
        title: 'üíæ Database Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Database:* {{ .Labels.service }}
          *Description:* {{ .Annotations.description }}
          {{ end }}
        color: 'warning'
  
  # Development team alerts
  - name: 'dev-team'
    email_configs:
      - to: 'developers@claude-manager.local'
        subject: '[Development] {{ .GroupLabels.alertname }}'
        body: |
          Development Alert Details:
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          {{ end }}
    
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#development'
        title: 'üíª Development Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Service:* {{ .Labels.service }}
          {{ if .Annotations.runbook_url }}*Runbook:* <{{ .Annotations.runbook_url }}|View>{{ end }}
          {{ end }}
        color: 'warning'
  
  # Product team alerts
  - name: 'product-team'
    email_configs:
      - to: 'product@claude-manager.local'
        subject: '[Product] {{ .GroupLabels.alertname }}'
        body: |
          Product/Business Alert Details:
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Impact: Business metrics affected
          {{ end }}
    
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#product'
        title: 'üìà Business Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Impact:* Business metrics may be affected
          *Description:* {{ .Annotations.description }}
          {{ end }}
        color: 'warning'

# Inhibition rules to reduce alert noise
inhibit_rules:
  # Inhibit warning alerts when critical alerts are firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service', 'instance']
  
  # Inhibit individual service alerts when application is down
  - source_match:
      alertname: 'ApplicationDown'
    target_match_re:
      alertname: '^(HighErrorRate|HighResponseTime|DatabaseConnectionFailure)$'
    equal: ['service']
  
  # Inhibit database connection alerts when database is down
  - source_match_re:
      alertname: '^(PostgreSQLDown|RedisDown)$'
    target_match:
      alertname: 'DatabaseConnectionFailure'
    equal: ['service']

# Silence configuration
mute_time_intervals:
  # Maintenance windows
  - name: 'maintenance-window'
    time_intervals:
      - times:
          - start_time: '02:00'
            end_time: '04:00'
        weekdays: ['sunday']
        months: ['1:12']
  
  # Business hours (reduce non-critical alert noise)
  - name: 'business-hours'
    time_intervals:
      - times:
          - start_time: '09:00'
            end_time: '17:00'
        weekdays: ['monday:friday']

# Template files for custom alert formatting
# Place custom templates in /etc/alertmanager/templates/